# Giáº£i thÃ­ch vá» MÃ´ hÃ¬nh Machine Learning

## ğŸ“Š Tá»•ng quan

MÃ´ hÃ¬nh nÃ y lÃ  má»™t **Random Forest Classifier** Ä‘Æ°á»£c sá»­ dá»¥ng cho bÃ i toÃ¡n **phÃ¢n loáº¡i nhá»‹ phÃ¢n** (binary classification).

## ğŸ¯ BÃ i toÃ¡n

**Má»¥c tiÃªu**: PhÃ¢n loáº¡i dá»¯ liá»‡u vÃ o 2 lá»›p (Class 0 hoáº·c Class 1) dá»±a trÃªn 10 Ä‘áº·c trÆ°ng (features).

## ğŸ“ˆ Dá»¯ liá»‡u

### Nguá»“n dá»¯ liá»‡u
- ÄÆ°á»£c táº¡o báº±ng hÃ m `make_classification` cá»§a scikit-learn
- **Sá»‘ lÆ°á»£ng máº«u**: 1,000 máº«u
- **Sá»‘ Ä‘áº·c trÆ°ng**: 10 features (feature_0 Ä‘áº¿n feature_9)
- **Sá»‘ Ä‘áº·c trÆ°ng cÃ³ Ã½ nghÄ©a**: 5 (n_informative=5)
- **Sá»‘ lá»›p**: 2 (binary classification)
- **Random state**: 42 (Ä‘á»ƒ Ä‘áº£m báº£o káº¿t quáº£ cÃ³ thá»ƒ tÃ¡i láº­p)

### PhÃ¢n chia dá»¯ liá»‡u
- **Training set**: 80% (800 máº«u)
- **Test set**: 20% (200 máº«u)
- **Random state**: 42

## ğŸŒ² MÃ´ hÃ¬nh: Random Forest Classifier

### Random Forest lÃ  gÃ¬?

Random Forest lÃ  má»™t thuáº­t toÃ¡n **ensemble learning** (há»c táº­p táº­p há»£p) hoáº¡t Ä‘á»™ng báº±ng cÃ¡ch:

1. **Táº¡o nhiá»u cÃ¢y quyáº¿t Ä‘á»‹nh** (Decision Trees) Ä‘á»™c láº­p
2. Má»—i cÃ¢y Ä‘Æ°á»£c train trÃªn má»™t táº­p con ngáº«u nhiÃªn cá»§a dá»¯ liá»‡u (bootstrap sampling)
3. Má»—i cÃ¢y chá»‰ xem xÃ©t má»™t táº­p con ngáº«u nhiÃªn cá»§a cÃ¡c features khi phÃ¢n chia
4. **Káº¿t há»£p káº¿t quáº£** tá»« táº¥t cáº£ cÃ¡c cÃ¢y báº±ng cÃ¡ch bá» phiáº¿u (voting)

### Táº¡i sao chá»n Random Forest?

âœ… **Æ¯u Ä‘iá»ƒm**:
- Xá»­ lÃ½ tá»‘t dá»¯ liá»‡u cÃ³ nhiá»u features
- KhÃ´ng cáº§n chuáº©n hÃ³a dá»¯ liá»‡u
- TrÃ¡nh overfitting tá»‘t hÆ¡n Decision Tree Ä‘Æ¡n láº»
- CÃ³ thá»ƒ xá»­ lÃ½ missing values
- Cho biáº¿t táº§m quan trá»ng cá»§a tá»«ng feature

## ğŸ”§ SiÃªu tham sá»‘ Ä‘Ã£ tuning

### n_estimators (Sá»‘ lÆ°á»£ng cÃ¢y trong rá»«ng)

ÄÃ£ thá»­ nghiá»‡m 3 giÃ¡ trá»‹:
- **50 cÃ¢y**: Accuracy = 0.940 (94.0%)
- **100 cÃ¢y**: Accuracy = 0.945 (94.5%)
- **200 cÃ¢y**: Accuracy = 0.950 (95.0%) â­ **Tá»T NHáº¤T**

### LÃ½ do tuning n_estimators:

1. **n_estimators = 50**: 
   - Nhanh hÆ¡n nhÆ°ng cÃ³ thá»ƒ chÆ°a Ä‘á»§ Ä‘á»ƒ há»c háº¿t patterns
   - Accuracy tháº¥p nháº¥t: 94.0%

2. **n_estimators = 100**:
   - CÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c
   - Accuracy: 94.5%

3. **n_estimators = 200**: 
   - Nhiá»u cÃ¢y hÆ¡n â†’ há»c Ä‘Æ°á»£c nhiá»u patterns hÆ¡n
   - Accuracy cao nháº¥t: 95.0%
   - ÄÆ°á»£c chá»n lÃ m mÃ´ hÃ¬nh tá»‘t nháº¥t

### CÃ¡c tham sá»‘ khÃ¡c (máº·c Ä‘á»‹nh):
- `random_state=42`: Äáº£m báº£o káº¿t quáº£ cÃ³ thá»ƒ tÃ¡i láº­p
- `max_depth=None`: KhÃ´ng giá»›i háº¡n Ä‘á»™ sÃ¢u cÃ¢y
- `min_samples_split=2`: Tá»‘i thiá»ƒu 2 máº«u Ä‘á»ƒ tÃ¡ch node
- `min_samples_leaf=1`: Má»—i leaf node cÃ³ Ã­t nháº¥t 1 máº«u

## ğŸ“Š Káº¿t quáº£ Training

| n_estimators | Accuracy | Ghi chÃº |
|--------------|----------|---------|
| 50 | 94.0% | Nhanh nhÆ°ng accuracy tháº¥p |
| 100 | 94.5% | CÃ¢n báº±ng tá»‘t |
| **200** | **95.0%** | **Tá»‘t nháº¥t - Ä‘Æ°á»£c chá»n** |

## ğŸ”„ Quy trÃ¬nh hoáº¡t Ä‘á»™ng

### 1. Training Phase (train.py)

```python
# BÆ°á»›c 1: Äá»c dá»¯ liá»‡u
data = pd.read_csv("data.csv")
X = data.drop("target", axis=1)  # 10 features
y = data["target"]                # 2 classes (0 hoáº·c 1)

# BÆ°á»›c 2: Chia train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# BÆ°á»›c 3: Training vá»›i cÃ¡c n_estimators khÃ¡c nhau
for n in [50, 100, 200]:
    model = RandomForestClassifier(n_estimators=n)
    model.fit(X_train, y_train)  # Há»c tá»« dá»¯ liá»‡u training
    
    # BÆ°á»›c 4: ÄÃ¡nh giÃ¡
    predictions = model.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)
    
    # BÆ°á»›c 5: LÆ°u vÃ o MLflow
    mlflow.log_model(model, name="model")
```

### 2. Model Selection (save_best_model.py)

- TÃ¬m mÃ´ hÃ¬nh cÃ³ accuracy cao nháº¥t trong MLflow
- Táº£i mÃ´ hÃ¬nh tá»‘t nháº¥t (n_estimators=200, accuracy=95.0%)
- LÆ°u vÃ o thÆ° má»¥c `best_model/`

### 3. Inference Phase (app.py)

```python
# Load mÃ´ hÃ¬nh
model = mlflow.sklearn.load_model("best_model")

# Nháº­n input tá»« user (10 features)
features = [feature_0, feature_1, ..., feature_9]

# Dá»± Ä‘oÃ¡n
prediction = model.predict([features])  # Class 0 hoáº·c 1
probabilities = model.predict_proba([features])  # XÃ¡c suáº¥t cho má»—i class
```

## ğŸ¯ CÃ¡ch mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n

### Input:
- 10 giÃ¡ trá»‹ sá»‘ thá»±c (features) tá»« ngÆ°á»i dÃ¹ng

### Quy trÃ¬nh:
1. **Má»—i cÃ¢y trong rá»«ng** (200 cÃ¢y) Ä‘Æ°a ra má»™t dá»± Ä‘oÃ¡n
2. **Bá» phiáº¿u**: Lá»›p nÃ o Ä‘Æ°á»£c nhiá»u cÃ¢y chá»n nháº¥t sáº½ tháº¯ng
3. **TÃ­nh xÃ¡c suáº¥t**: Tá»· lá»‡ sá»‘ cÃ¢y bá» phiáº¿u cho má»—i lá»›p

### Output:
- **Prediction**: Lá»›p dá»± Ä‘oÃ¡n (0 hoáº·c 1)
- **Probability**: XÃ¡c suáº¥t cho má»—i lá»›p
  - P(Class 0): XÃ¡c suáº¥t máº«u thuá»™c lá»›p 0
  - P(Class 1): XÃ¡c suáº¥t máº«u thuá»™c lá»›p 1

### VÃ­ dá»¥:
```
Input: [1.125, 1.178, 0.494, 0.791, -0.614, 1.347, 1.420, 1.357, 0.966, -1.981]

QuÃ¡ trÃ¬nh:
- CÃ¢y 1: Dá»± Ä‘oÃ¡n Class 1
- CÃ¢y 2: Dá»± Ä‘oÃ¡n Class 1
- CÃ¢y 3: Dá»± Ä‘oÃ¡n Class 0
- ...
- CÃ¢y 200: Dá»± Ä‘oÃ¡n Class 1

Káº¿t quáº£ bá» phiáº¿u:
- Class 0: 10 cÃ¢y (5%)
- Class 1: 190 cÃ¢y (95%)

Output:
- Prediction: 1
- Probability Class 0: 0.05 (5%)
- Probability Class 1: 0.95 (95%)
```

## ğŸŒ á»¨ng dá»¥ng Web (app.py)

### Luá»“ng hoáº¡t Ä‘á»™ng:

1. **Khá»Ÿi Ä‘á»™ng**: Load mÃ´ hÃ¬nh tá»‘t nháº¥t tá»« `best_model/`
2. **NgÆ°á»i dÃ¹ng nháº­p**: 10 giÃ¡ trá»‹ features qua form web
3. **Xá»­ lÃ½**: 
   - Chuyá»ƒn Ä‘á»•i input thÃ nh numpy array
   - Reshape thÃ nh shape (1, 10) - 1 máº«u, 10 features
4. **Dá»± Ä‘oÃ¡n**:
   - Gá»i `model.predict()` â†’ Lá»›p dá»± Ä‘oÃ¡n
   - Gá»i `model.predict_proba()` â†’ XÃ¡c suáº¥t
5. **Hiá»ƒn thá»‹**: Tráº£ vá» káº¿t quáº£ cho ngÆ°á»i dÃ¹ng

### API Endpoints:

- **GET `/`**: Form nháº­p liá»‡u
- **POST `/predict`**: API dá»± Ä‘oÃ¡n
- **GET `/health`**: Kiá»ƒm tra tráº¡ng thÃ¡i

## ğŸ“ TÃ³m táº¯t

| ThÃ´ng tin | GiÃ¡ trá»‹ |
|-----------|---------|
| **Thuáº­t toÃ¡n** | Random Forest Classifier |
| **Sá»‘ cÃ¢y** | 200 (n_estimators) |
| **Sá»‘ features** | 10 |
| **Sá»‘ classes** | 2 (binary) |
| **Accuracy** | 95.0% |
| **Dá»¯ liá»‡u training** | 800 máº«u |
| **Dá»¯ liá»‡u test** | 200 máº«u |
| **Framework** | scikit-learn + MLflow |
| **Deployment** | Flask Web Application |

## ğŸ” ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh

### Äiá»ƒm máº¡nh:
- âœ… Accuracy cao (95%)
- âœ… á»”n Ä‘á»‹nh (ensemble method)
- âœ… CÃ³ thá»ƒ giáº£i thÃ­ch Ä‘Æ°á»£c (feature importance)
- âœ… Xá»­ lÃ½ tá»‘t dá»¯ liá»‡u khÃ´ng cáº§n chuáº©n hÃ³a

### Háº¡n cháº¿:
- âš ï¸ CÃ³ thá»ƒ cháº­m hÆ¡n vá»›i sá»‘ lÆ°á»£ng cÃ¢y lá»›n
- âš ï¸ Cáº§n nhiá»u bá»™ nhá»› Ä‘á»ƒ lÆ°u trá»¯ nhiá»u cÃ¢y
- âš ï¸ KhÃ³ giáº£i thÃ­ch chi tiáº¿t hÆ¡n so vá»›i Decision Tree Ä‘Æ¡n láº»

## ğŸš€ Cáº£i thiá»‡n cÃ³ thá»ƒ thá»±c hiá»‡n

1. **Tuning thÃªm hyperparameters**:
   - `max_depth`: Giá»›i háº¡n Ä‘á»™ sÃ¢u cÃ¢y
   - `min_samples_split`: Sá»‘ máº«u tá»‘i thiá»ƒu Ä‘á»ƒ tÃ¡ch node
   - `max_features`: Sá»‘ features xem xÃ©t khi tÃ¡ch node

2. **Feature Engineering**:
   - Táº¡o thÃªm features má»›i
   - Chá»n lá»c features quan trá»ng

3. **Thá»­ cÃ¡c thuáº­t toÃ¡n khÃ¡c**:
   - Gradient Boosting (XGBoost, LightGBM)
   - Support Vector Machine (SVM)
   - Neural Networks

4. **Cross-validation**:
   - Sá»­ dá»¥ng k-fold cross-validation Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ tá»‘t hÆ¡n

